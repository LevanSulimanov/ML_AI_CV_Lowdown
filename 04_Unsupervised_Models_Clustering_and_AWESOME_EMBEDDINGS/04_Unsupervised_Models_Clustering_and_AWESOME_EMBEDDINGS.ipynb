{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f43c2c",
   "metadata": {},
   "source": [
    "# Classwork: Pre-training vs. Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f191aa",
   "metadata": {},
   "source": [
    "\n",
    "In this classwork, you will explore **pre-training** and **fine-tuning** by fine-tuning a pre-trained model for a sentiment analysis task. \n",
    "We will use the IMDB dataset and TensorFlow/Keras for implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee567937",
   "metadata": {},
   "source": [
    "## Step 1: Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8a4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load IMDB dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "X_train = pad_sequences(X_train, maxlen=200)\n",
    "X_test = pad_sequences(X_test, maxlen=200)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}, Test set size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c879709",
   "metadata": {},
   "source": [
    "## Step 2: Define and Load a Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define a model with an embedding layer\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=200),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Display the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a59e35",
   "metadata": {},
   "source": [
    "## Step 3: Fine-tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a235a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=3, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050e5ca",
   "metadata": {},
   "source": [
    "## Reflection Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1578f",
   "metadata": {},
   "source": [
    "\n",
    "1. How does the performance compare to what you would expect from training a model from scratch?\n",
    "2. What are the advantages of using pre-trained models in this context?\n",
    "3. How might the embedding layer improve generalization?\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
